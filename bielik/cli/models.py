#!/usr/bin/env python3
"""
Bielik CLI Model Manager - Handles Hugging Face model management commands.

This module provides CLI commands for downloading, managing, and switching
between SpeakLeash models from Hugging Face.
"""

from typing import Dict, Any
from pathlib import Path

from ..hf_models import get_model_manager, HAS_LLAMA_CPP
from ..config import get_config, get_logger


class ModelManager:
    """Manages CLI model operations for Hugging Face models."""
    
    def __init__(self):
        self.config = get_config()
        self.logger = get_logger(__name__)
        self.model_manager = get_model_manager()
    
    def show_models(self) -> None:
        """Display available and downloaded Hugging Face models."""
        print("\nðŸ¤— Hugging Face Models - SpeakLeash:")
        print("=" * 50)
        
        # Show available models
        available = self.model_manager.list_available_models()
        print("ðŸ“‹ Available Models:")
        for model_name, info in available.items():
            status = "âœ… Downloaded" if self.model_manager.is_model_downloaded(model_name) else "â¬‡ï¸ Available for download"
            print(f"  {model_name}")
            print(f"    ðŸ“ {info['description']}")
            print(f"    ðŸ“Š Parameters: {info['parameters']}")
            print(f"    ðŸ“ˆ Status: {status}")
            print()
        
        # Show downloaded models
        downloaded = self.model_manager.list_downloaded_models()
        if downloaded:
            print("ðŸ’¾ Downloaded Models:")
            for model_name, info in downloaded.items():
                size_gb = info.size_bytes / (1024**3)
                print(f"  {model_name} ({size_gb:.1f} GB)")
                print(f"    ðŸ“ Path: {info.local_path}")
            print()
    
    def download_model(self, model_name: str) -> bool:
        """Download a Hugging Face model and return success status."""
        if model_name not in self.model_manager.SPEAKLEASH_MODELS:
            print(f"âŒ Unknown model: {model_name}")
            print("ðŸ’¡ Use :models to see available models")
            return False
        
        if self.model_manager.is_model_downloaded(model_name):
            print(f"â„¹ï¸  Model {model_name} is already downloaded.")
            try:
                force = input("Download again? (y/N): ").lower()
                if force not in ['y', 'yes']:
                    return False
                force_download = True
            except (EOFError, KeyboardInterrupt):
                print("\nâŒ Download cancelled")
                return False
        else:
            force_download = False
        
        print(f"â¬‡ï¸ Downloading model {model_name}...")
        try:
            model_info = self.model_manager.download_model(model_name, force=force_download)
            if model_info:
                size_gb = model_info.size_bytes / (1024**3)
                print(f"âœ… Model downloaded successfully! ({size_gb:.1f} GB)")
                print(f"ðŸ“ Path: {model_info.local_path}")
                return True
            else:
                print("âŒ Failed to download model")
                return False
        except Exception as e:
            self.logger.error(f"Error downloading model: {e}")
            print(f"âŒ Download error: {e}")
            return False
    
    def delete_model(self, model_name: str) -> None:
        """Delete a downloaded Hugging Face model."""
        if not self.model_manager.is_model_downloaded(model_name):
            print(f"âŒ Model {model_name} is not downloaded")
            return
        
        # Get model info for confirmation
        downloaded = self.model_manager.list_downloaded_models()
        if model_name in downloaded:
            size_gb = downloaded[model_name].get('size_bytes', 0) / (1024**3)
            print(f"ðŸ—‘ï¸  Are you sure you want to delete model {model_name} ({size_gb:.1f} GB)?")
            try:
                confirm = input("Confirm deletion (y/N): ").lower()
                if confirm not in ['y', 'yes']:
                    print("âŒ Deletion cancelled")
                    return
            except (EOFError, KeyboardInterrupt):
                print("\nâŒ Deletion cancelled")
                return
        
        try:
            if self.model_manager.delete_model(model_name):
                print(f"âœ… Model {model_name} deleted successfully")
            else:
                print(f"âŒ Failed to delete model {model_name}")
        except Exception as e:
            self.logger.error(f"Error deleting model: {e}")
            print(f"âŒ Deletion error: {e}")
    
    def show_storage_stats(self) -> None:
        """Show storage statistics for HF models."""
        print("\nðŸ’¾ Storage Statistics:")
        print("=" * 30)
        
        try:
            stats = self.model_manager.get_storage_stats()
            print(f"ðŸ“Š Downloaded models: {stats['downloaded_count']}")
            print(f"ðŸ’½ Total size: {stats['total_size_gb']:.1f} GB")
            print(f"ðŸ“ Models directory: {stats['models_dir']}")
            
            if stats['models']:
                print("\nðŸ“‹ Model Details:")
                for model_name, info in stats['models'].items():
                    size_gb = info['size_bytes'] / (1024**3)
                    print(f"  {model_name}: {size_gb:.1f} GB")
        except Exception as e:
            self.logger.error(f"Error getting storage stats: {e}")
            print(f"âŒ Error retrieving statistics: {e}")
    
    def switch_model(self, model_name: str, current_model: str) -> str:
        """
        Switch to a different model (HF or Ollama).
        
        Args:
            model_name: Name of the model to switch to
            current_model: Currently active model name
            
        Returns:
            New model name to use
        """
        # Check if it's an HF model
        if model_name in self.model_manager.SPEAKLEASH_MODELS:
            if not self.model_manager.is_model_downloaded(model_name):
                print(f"âŒ Model {model_name} is not downloaded")
                print(f"ðŸ’¡ Use :download {model_name} to download it")
                return current_model
            
            # Allow switching even without llama-cpp-python (minimal version)
            # Local execution will be handled elsewhere with appropriate fallbacks
            print(f"ðŸ”„ Switched to HF model: {model_name}")
            if not HAS_LLAMA_CPP:
                print("ðŸ’¡ Note: Local execution requires: pip install 'bielik[local]'")
                print("ðŸ’¡ Model preference saved - will use HF API if available")
            else:
                print("ðŸ’¡ Model will be used for local execution")
            return model_name
            
        else:
            # Assume it's an Ollama model - we can't easily check this without setup manager
            print(f"ðŸ”„ Switched to Ollama model: {model_name}")
            print("ðŸ’¡ Make sure the model is installed in Ollama")
            return model_name
    
    def get_model_info(self, model_name: str) -> Dict[str, Any]:
        """Get information about a specific model."""
        info = {
            "model_name": model_name,
            "is_hf_model": model_name in self.model_manager.SPEAKLEASH_MODELS,
            "is_downloaded": False,
            "has_llama_cpp": HAS_LLAMA_CPP
        }
        
        if info["is_hf_model"]:
            info["is_downloaded"] = self.model_manager.is_model_downloaded(model_name)
            if info["is_downloaded"]:
                downloaded = self.model_manager.list_downloaded_models()
                if model_name in downloaded:
                    model_info = downloaded[model_name]
                    info.update({
                        "local_path": model_info.local_path,
                        "size_gb": model_info.size_bytes / (1024**3),
                        "description": self.model_manager.SPEAKLEASH_MODELS[model_name]["description"],
                        "parameters": self.model_manager.SPEAKLEASH_MODELS[model_name]["parameters"]
                    })
        
        return info
    
    def get_full_model_name(self, model_name: str) -> str:
        """
        Get the full model name for Ollama usage.
        
        For HF models, this returns the model name as-is since they're used directly.
        For Ollama models, this would be the complete model identifier.
        
        Args:
            model_name: Short model name
            
        Returns:
            Full model name for usage in chat
        """
        # For HF models, return as-is
        if model_name in self.model_manager.SPEAKLEASH_MODELS:
            return model_name
        
        # For Ollama models, assume the name is already correct
        # In a more sophisticated implementation, we could validate against Ollama API
        return model_name
