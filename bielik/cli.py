#!/usr/bin/env python3
"""
Bielik CLI - Interactive chat shell with modular architecture.

This module serves as the main entry point for the Bielik CLI application.
It now uses a modular architecture with separate components for setup,
commands, model management, and chat communication.

All strings and comments have been converted to English as requested.

The original 716-line monolithic CLI has been refactored into:
- cli/main.py: Main entry point and argument parsing
- cli/commands.py: Command processing and help
- cli/setup.py: Interactive setup and system configuration  
- cli/models.py: HF model management commands
- cli/send_chat.py: Chat communication layer
"""

from .cli.main import main

# Backward compatibility exports
from .cli.send_chat import send_chat
from .cli.commands import CommandProcessor
from .cli.setup import SetupManager
from .cli.models import ModelManager as CLIModelManager

__all__ = ['main', 'send_chat', 'CommandProcessor', 'SetupManager', 'CLIModelManager']
    """Send chat messages to Ollama or local HF model."""
    if model is None:
        model = config.BIELIK_MODEL
    
    # Use local HF model if requested and available
    if use_local or model in model_manager.SPEAKLEASH_MODELS:
        if HAS_LLAMA_CPP and model_manager.is_model_downloaded(model):
            try:
                from .hf_models import LocalLlamaRunner
                
                # Get model path
                model_path = model_manager.get_model_path(model)
                if not model_path:
                    return f"[LOCAL MODEL ERROR] Model {model} not found locally"
                
                # Initialize local runner
                runner = LocalLlamaRunner(model_path)
                response = runner.chat(messages)
                logger.info(f"Local HF model response received for {model}")
                return response
                
            except Exception as e:
                logger.error(f"Local HF model failed for {model}: {e}")
                if not use_local:  # Fall through to Ollama if not explicitly using local
                    logger.info("Falling back to Ollama...")
                else:
                    return f"[LOCAL MODEL ERROR] {e}"
        elif use_local:
            if not HAS_LLAMA_CPP:
                return "[LOCAL MODEL ERROR] llama-cpp-python not installed"
            elif not model_manager.is_model_downloaded(model):
                return f"[LOCAL MODEL ERROR] Model {model} not downloaded. Use :download {model}"
    
    # Use Ollama (REST API first, then library fallback)
    payload = {"model": model, "messages": messages, "stream": False}

    # try REST first
    try:
        resp = requests.post(config.CHAT_ENDPOINT, json=payload, timeout=config.REQUEST_TIMEOUT)
        resp.raise_for_status()
        data = resp.json()
        choice = data.get("choices", [{}])[0]
        msg = choice.get("message", {}).get("content")
        if msg:
            logger.debug(f"REST API successful for model {model}")
            return msg
    except Exception as e:
        logger.warning(f"REST API failed: {e}")

    # fallback: official ollama library
    if HAVE_OLLAMA:
        try:
            response = ollama.chat(model=model, messages=messages)
            logger.debug(f"Ollama library successful for model {model}")
            return response["message"]["content"]
        except Exception as e:
            logger.error(f"Ollama library failed: {e}")
            return f"[OLLAMA LIB ERROR] {e}"

    logger.error("No working Ollama integration available")
    return "[ERROR] No working Ollama integration (REST and lib failed)."


def show_welcome():
    """Display welcome message and help for beginners."""
    print("ü¶Ö " + "="*50)
    print("   BIELIK - Polski Asystent AI")
    print("   Powered by Ollama + Speakleash")
    print("="*53)
    print()
    print("üìã Dostƒôpne komendy:")
    print("  :help    - poka≈º tƒô pomoc")
    print("  :status  - sprawd≈∫ po≈ÇƒÖczenie z Ollama")
    print("  :setup   - uruchom interaktywnƒÖ konfiguracjƒô")
    print("  :clear   - wyczy≈õƒá historiƒô rozmowy")
    print("  :models  - poka≈º dostƒôpne modele HF")
    print("  :download <model> - pobierz model z Hugging Face")
    print("  :delete <model>   - usu≈Ñ pobrany model")
    print("  :switch <model>   - prze≈ÇƒÖcz na model")
    print("  :storage - poka≈º statystyki pamiƒôci")
    print("  :exit    - zako≈Ñcz sesjƒô")
    print("  Ctrl+C   - szybkie wyj≈õcie")
    print()
    print("üí° Wskaz√≥wki:")
    print("  ‚Ä¢ Pisz po polsku - Bielik rozumie jƒôzyk polski!")
    print("  ‚Ä¢ Zadawaj pytania, pro≈õ o pomoc, rozmawiaj naturalnie")
    print("  ‚Ä¢ Je≈õli Ollama nie dzia≈Ça, zobaczysz komunikat o b≈Çƒôdzie")
    print()


def is_ollama_installed():
    """Check if Ollama binary is installed on the system."""
    return shutil.which("ollama") is not None


def is_ollama_running():
    """Check if Ollama server is running."""
    try:
        resp = requests.get(f"{OLLAMA_HOST.rstrip('/')}/api/tags", timeout=5)
        return resp.status_code == 200
    except Exception:
        return False


def get_installed_models():
    """Get list of installed Ollama models."""
    try:
        resp = requests.get(f"{OLLAMA_HOST.rstrip('/')}/api/tags", timeout=5)
        if resp.status_code == 200:
            data = resp.json()
            return [m['name'] for m in data.get('models', [])]
    except Exception:
        pass
    return []


def is_bielik_model_available():
    """Check if Bielik model is available."""
    models = get_installed_models()
    return any('bielik' in model.lower() for model in models)


def install_ollama():
    """Install Ollama based on the operating system."""
    system = platform.system().lower()
    
    print("üîß Instalowanie Ollama...")
    
    try:
        if system == "linux":
            print("üì¶ Wykryto Linux - u≈ºywam curl do pobrania Ollama...")
            cmd = ["curl", "-fsSL", "https://ollama.com/install.sh"]
            result = subprocess.run(cmd, capture_output=True, text=True, timeout=60)
            if result.returncode == 0:
                # Run the install script
                install_cmd = ["sh", "-c", result.stdout]
                subprocess.run(install_cmd, check=True, timeout=300)
                print("‚úÖ Ollama zainstalowana pomy≈õlnie!")
                return True
            else:
                print(f"‚ùå B≈ÇƒÖd pobierania skryptu instalacji: {result.stderr}")
                return False
                
        elif system == "darwin":  # macOS
            print("üì¶ Wykryto macOS - sprawdzam Homebrew...")
            if shutil.which("brew"):
                subprocess.run(["brew", "install", "ollama"], check=True, timeout=300)
                print("‚úÖ Ollama zainstalowana przez Homebrew!")
                return True
            else:
                print("‚ùå Homebrew nie znaleziony. Zainstaluj rƒôcznie z https://ollama.com")
                return False
                
        elif system == "windows":
            print("üì¶ Wykryto Windows - wymagana instalacja rƒôczna")
            print("üìñ Pobierz Ollama z: https://ollama.com/download/windows")
            print("   Uruchom pobrany plik .exe i postƒôpuj zgodnie z instrukcjami")
            return False
            
        else:
            print(f"‚ùå Nieobs≈Çugiwany system operacyjny: {system}")
            return False
            
    except subprocess.TimeoutExpired:
        print("‚ùå Instalacja przekroczy≈Ça limit czasu")
        return False
    except subprocess.CalledProcessError as e:
        print(f"‚ùå B≈ÇƒÖd podczas instalacji: {e}")
        return False
    except Exception as e:
        print(f"‚ùå Nieoczekiwany b≈ÇƒÖd: {e}")
        return False


def start_ollama_server():
    """Start Ollama server in background."""
    try:
        print("üöÄ Uruchamianie serwera Ollama...")
        if platform.system().lower() == "windows":
            subprocess.Popen(["ollama", "serve"], creationflags=subprocess.CREATE_NEW_CONSOLE)
        else:
            subprocess.Popen(["ollama", "serve"], stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)
        
        # Wait a bit for server to start
        import time
        time.sleep(3)
        
        if is_ollama_running():
            print("‚úÖ Serwer Ollama uruchomiony!")
            return True
        else:
            print("‚ö†Ô∏è Serwer mo≈ºe potrzebowaƒá wiƒôcej czasu na uruchomienie...")
            return False
    except Exception as e:
        print(f"‚ùå B≈ÇƒÖd uruchamiania serwera: {e}")
        return False


def pull_bielik_model():
    """Download and install Bielik model."""
    try:
        print(f"üì• Pobieranie modelu {DEFAULT_MODEL}...")
        print("‚è≥ To mo≈ºe potrwaƒá kilka minut w zale≈ºno≈õci od po≈ÇƒÖczenia...")
        
        result = subprocess.run(
            ["ollama", "pull", DEFAULT_MODEL], 
            capture_output=True, 
            text=True, 
            timeout=1800  # 30 minutes timeout
        )
        
        if result.returncode == 0:
            print("‚úÖ Model Bielik pobrany pomy≈õlnie!")
            return True
        else:
            print(f"‚ùå B≈ÇƒÖd pobierania modelu: {result.stderr}")
            return False
            
    except subprocess.TimeoutExpired:
        print("‚ùå Pobieranie modelu przekroczy≈Ço limit czasu (30 min)")
        return False
    except Exception as e:
        print(f"‚ùå B≈ÇƒÖd pobierania modelu: {e}")
        return False


def show_hf_models():
    """Display available and downloaded Hugging Face models."""
    print("\nü§ó Modele Hugging Face - SpeakLeash:")
    print("=" * 50)
    
    # Show available models
    available = model_manager.list_available_models()
    print("üìã Dostƒôpne modele:")
    for model_name, info in available.items():
        status = "‚úÖ Pobrany" if model_manager.is_model_downloaded(model_name) else "‚¨áÔ∏è Do pobrania"
        print(f"  {model_name}")
        print(f"    üìù {info['description']}")
        print(f"    üìä Parametry: {info['parameters']}")
        print(f"    üìà Status: {status}")
        print()
    
    # Show downloaded models
    downloaded = model_manager.list_downloaded_models()
    if downloaded:
        print("üíæ Pobrane modele:")
        for model_name, info in downloaded.items():
            size_gb = info.get('size_bytes', 0) / (1024**3)
            print(f"  {model_name} ({size_gb:.1f} GB)")
            print(f"    üìÅ ≈öcie≈ºka: {info.get('local_path', 'N/A')}")
        print()


def download_hf_model(model_name: str):
    """Download a Hugging Face model."""
    if model_name not in model_manager.SPEAKLEASH_MODELS:
        print(f"‚ùå Nieznany model: {model_name}")
        print("üí° U≈ºyj :models aby zobaczyƒá dostƒôpne modele")
        return
    
    if model_manager.is_model_downloaded(model_name):
        print(f"‚ÑπÔ∏è  Model {model_name} jest ju≈º pobrany.")
        force = input("Czy pobraƒá ponownie? (t/N): ").lower()
        if force not in ['t', 'tak', 'y', 'yes']:
            return
        force_download = True
    else:
        force_download = False
    
    print(f"‚¨áÔ∏è Pobieranie modelu {model_name}...")
    try:
        model_info = model_manager.download_model(model_name, force=force_download)
        if model_info:
            size_gb = model_info.size_bytes / (1024**3)
            print(f"‚úÖ Model pobrany pomy≈õlnie! ({size_gb:.1f} GB)")
            print(f"üìÅ ≈öcie≈ºka: {model_info.local_path}")
        else:
            print("‚ùå Nie uda≈Ço siƒô pobraƒá modelu")
    except Exception as e:
        logger.error(f"Error downloading model: {e}")
        print(f"‚ùå B≈ÇƒÖd pobierania: {e}")


def delete_hf_model(model_name: str):
    """Delete a downloaded Hugging Face model."""
    if not model_manager.is_model_downloaded(model_name):
        print(f"‚ùå Model {model_name} nie jest pobrany")
        return
    
    # Get model info for confirmation
    downloaded = model_manager.list_downloaded_models()
    if model_name in downloaded:
        size_gb = downloaded[model_name].get('size_bytes', 0) / (1024**3)
        print(f"üóëÔ∏è  Czy na pewno usunƒÖƒá model {model_name} ({size_gb:.1f} GB)?")
        confirm = input("Potwierd≈∫ (t/N): ").lower()
        if confirm not in ['t', 'tak', 'y', 'yes']:
            print("‚ùå Anulowano usuwanie")
            return
    
    try:
        if model_manager.delete_model(model_name):
            print(f"‚úÖ Model {model_name} usuniƒôty pomy≈õlnie")
        else:
            print(f"‚ùå Nie uda≈Ço siƒô usunƒÖƒá modelu {model_name}")
    except Exception as e:
        logger.error(f"Error deleting model: {e}")
        print(f"‚ùå B≈ÇƒÖd usuwania: {e}")


def show_storage_stats():
    """Show storage statistics for HF models."""
    print("\nüíæ Statystyki pamiƒôci:")
    print("=" * 30)
    
    try:
        stats = model_manager.get_storage_stats()
        print(f"üìä Pobrane modele: {stats['downloaded_count']}")
        print(f"üíΩ Ca≈Çkowity rozmiar: {stats['total_size_gb']:.1f} GB")
        print(f"üìÅ Katalog modeli: {stats['models_dir']}")
        
        if stats['models']:
            print("\nüìã Szczeg√≥≈Çy modeli:")
            for model_name, info in stats['models'].items():
                size_gb = info['size_bytes'] / (1024**3)
                print(f"  {model_name}: {size_gb:.1f} GB")
    except Exception as e:
        logger.error(f"Error getting storage stats: {e}")
        print(f"‚ùå B≈ÇƒÖd pobierania statystyk: {e}")


def switch_model(model_name: str):
    """Switch to a different model (HF or Ollama)."""
    # Check if it's an HF model
    if model_name in model_manager.SPEAKLEASH_MODELS:
        if not model_manager.is_model_downloaded(model_name):
            print(f"‚ùå Model {model_name} nie jest pobrany")
            print("üí° U≈ºyj :download {model_name} aby go pobraƒá")
            return
        
        if not HAS_LLAMA_CPP:
            print("‚ùå llama-cpp-python nie jest zainstalowane")
            print("üí° Zainstaluj: pip install llama-cpp-python")
            return
        
        # Update global model (this is a simple demo - in real app you'd update client)
        global DEFAULT_MODEL
        DEFAULT_MODEL = model_name
        print(f"üîÑ Prze≈ÇƒÖczono na lokalny model HF: {model_name}")
        print("üí° Model bƒôdzie u≈ºywany przy nastƒôpnych zapytaniach")
        
    else:
        # Assume it's an Ollama model
        models = get_installed_models()
        if model_name not in models:
            print(f"‚ùå Model Ollama {model_name} nie jest zainstalowany")
            print("üí° Zainstaluj: ollama pull {model_name}")
            return
        
        DEFAULT_MODEL = model_name
        print(f"üîÑ Prze≈ÇƒÖczono na model Ollama: {model_name}")


def interactive_setup():
    """Interactive setup process for first-time users."""
    print("üîç Sprawdzanie konfiguracji systemu...")
    print()
    
    setup_needed = False
    
    # Check Ollama installation
    if not is_ollama_installed():
        print("‚ùå Ollama nie jest zainstalowana")
        setup_needed = True
        
        try:
            install_choice = input("ü§î Czy zainstalowaƒá Ollama automatycznie? (t/N): ").strip().lower()
            if install_choice in ['t', 'tak', 'y', 'yes']:
                if install_ollama():
                    print("‚úÖ Ollama zainstalowana!")
                else:
                    print("‚ùå Instalacja nie powiod≈Ça siƒô. Zainstaluj rƒôcznie z https://ollama.com")
                    return False
            else:
                print("üìñ Zainstaluj Ollama rƒôcznie z: https://ollama.com")
                return False
        except (EOFError, KeyboardInterrupt):
            print("\n‚ùå Konfiguracja przerwana przez u≈ºytkownika")
            return False
    else:
        print("‚úÖ Ollama jest zainstalowana")
    
    # Check if Ollama server is running
    if not is_ollama_running():
        print("‚ùå Serwer Ollama nie dzia≈Ça")
        setup_needed = True
        
        try:
            start_choice = input("ü§î Czy uruchomiƒá serwer Ollama? (t/N): ").strip().lower()
            if start_choice in ['t', 'tak', 'y', 'yes']:
                if not start_ollama_server():
                    print("‚ùå Nie uda≈Ço siƒô uruchomiƒá serwera. Spr√≥buj rƒôcznie: ollama serve")
                    return False
            else:
                print("üìñ Uruchom serwer rƒôcznie: ollama serve")
                return False
        except (EOFError, KeyboardInterrupt):
            print("\n‚ùå Konfiguracja przerwana przez u≈ºytkownika")
            return False
    else:
        print("‚úÖ Serwer Ollama dzia≈Ça")
    
    # Check if Bielik model is available
    if not is_bielik_model_available():
        print(f"‚ùå Model {DEFAULT_MODEL} nie jest dostƒôpny")
        installed_models = get_installed_models()
        if installed_models:
            print(f"üìã Dostƒôpne modele: {', '.join(installed_models[:5])}")
        setup_needed = True
        
        try:
            model_choice = input(f"ü§î Czy pobraƒá model {DEFAULT_MODEL}? (t/N): ").strip().lower()
            if model_choice in ['t', 'tak', 'y', 'yes']:
                if not pull_bielik_model():
                    print(f"‚ùå Nie uda≈Ço siƒô pobraƒá modelu. Spr√≥buj rƒôcznie: ollama pull {DEFAULT_MODEL}")
                    return False
            else:
                print(f"üìñ Pobierz model rƒôcznie: ollama pull {DEFAULT_MODEL}")
                return False
        except (EOFError, KeyboardInterrupt):
            print("\n‚ùå Konfiguracja przerwana przez u≈ºytkownika")
            return False
    else:
        print(f"‚úÖ Model {DEFAULT_MODEL} jest dostƒôpny")
    
    if setup_needed:
        print()
        print("üéâ Konfiguracja zako≈Ñczona pomy≈õlnie!")
        print("üöÄ Bielik jest gotowy do u≈ºycia!")
    else:
        print("‚úÖ Wszystkie komponenty sƒÖ ju≈º skonfigurowane!")
    
    return True


def check_ollama_status():
    """Check if Ollama is running and accessible."""
    try:
        # Quick health check
        resp = requests.get(f"{config.OLLAMA_HOST.rstrip('/')}/api/tags", timeout=5)
        if resp.status_code == 200:
            data = resp.json()
            models = [m['name'] for m in data.get('models', [])]
            target_model = config.BIELIK_MODEL
            if any(target_model.lower() in model.lower() for model in models):
                logger.info(f"Ollama status check: Model {target_model} available")
                return f"‚úÖ Ollama running, model {target_model} available"
            else:
                available = ', '.join(models[:3]) if models else "none"
                logger.warning(f"Ollama running but model {target_model} not found")
                return f"‚ö†Ô∏è Ollama running, but model '{target_model}' missing. Available: {available}"
        else:
            logger.error(f"Ollama HTTP error: {resp.status_code}")
            return f"‚ùå Ollama responds but HTTP error {resp.status_code}"
    except Exception as e:
        logger.error(f"Ollama connection failed: {e}")
        return f"‚ùå Ollama unavailable: {str(e)}"


def parse_args():
    """Parse command line arguments."""
    parser = argparse.ArgumentParser(
        description="Bielik - Polski Asystent AI",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Przyk≈Çady u≈ºycia:
  bielik                                    # Standardowa sesja
  bielik --model other-model               # U≈ºyj innego modelu
  bielik --setup                           # Uruchom interaktywnƒÖ konfiguracjƒô
  bielik --no-setup                        # Pomi≈Ñ automatycznƒÖ konfiguracjƒô
        """
    )
    
    parser.add_argument(
        "--model", 
        default=DEFAULT_MODEL,
        help=f"Model do u≈ºycia (domy≈õlny: {DEFAULT_MODEL})"
    )
    
    parser.add_argument(
        "--setup", 
        action="store_true",
        help="Uruchom interaktywnƒÖ konfiguracjƒô systemu"
    )
    
    parser.add_argument(
        "--no-setup", 
        action="store_true",
        help="Pomi≈Ñ automatycznƒÖ konfiguracjƒô przy problemach"
    )
    
    parser.add_argument(
        "--host",
        default=OLLAMA_HOST,
        help=f"Adres serwera Ollama (domy≈õlny: {OLLAMA_HOST})"
    )
    
    parser.add_argument(
        "--use-local",
        action="store_true",
        help="U≈ºyj lokalnego modelu HF zamiast Ollama"
    )
    
    parser.add_argument(
        "--local-model",
        help="Nazwa lokalnego modelu HF do u≈ºycia"
    )
    
    return parser.parse_args()


def main():
    args = parse_args()
    
    # Override defaults with command line arguments
    global DEFAULT_MODEL, OLLAMA_HOST, CHAT_ENDPOINT
    DEFAULT_MODEL = args.model
    OLLAMA_HOST = args.host
    CHAT_ENDPOINT = OLLAMA_HOST.rstrip("/") + "/v1/chat/completions"
    
    # Handle local model arguments
    use_local_model = args.use_local
    if args.local_model:
        DEFAULT_MODEL = args.local_model
        use_local_model = True
    
    show_welcome()
    
    # Force setup if requested
    if args.setup:
        print("üîß Uruchamianie interaktywnej konfiguracji...")
        print()
        if not interactive_setup():
            print("‚ùå Konfiguracja nie powiod≈Ça siƒô.")
            return
        print()

    # Check Ollama status at startup
    status = check_ollama_status()
    print(f"üîó Status: {status}")
    print()

    # Handle problems with automatic setup unless disabled
    if "‚ùå" in status and not args.no_setup:
        print("üö® UWAGA: Wykryto problemy z konfiguracjƒÖ!")
        print()
        
        try:
            setup_choice = input("ü§î Czy uruchomiƒá automatycznƒÖ konfiguracjƒô? (T/n): ").strip().lower()
            if setup_choice not in ['n', 'nie', 'no']:
                print()
                if interactive_setup():
                    # Recheck status after setup
                    status = check_ollama_status()
                    print(f"üîó Nowy status: {status}")
                    print()
                else:
                    print("‚ùå Automatyczna konfiguracja nie powiod≈Ça siƒô.")
        except (EOFError, KeyboardInterrupt):
            print("\n‚ùå Konfiguracja przerwana przez u≈ºytkownika")
            return

    # If still having issues, offer to continue anyway
    if "‚ùå" in status:
        print("üìñ Instrukcje rƒôcznej naprawy:")
        print("  1. Zainstaluj Ollama: https://ollama.com")
        print("  2. Uruchom: ollama serve")
        print(f"  3. Zainstaluj model: ollama pull {DEFAULT_MODEL}")
        print("  4. Sprawd≈∫: ollama list")
        print()

        try:
            continue_anyway = input("Czy kontynuowaƒá mimo problem√≥w? (t/N): ")
            if continue_anyway.lower() not in ['t', 'tak', 'y', 'yes']:
                print("Sesja przerwana. Napraw konfiguracjƒô i spr√≥buj ponownie.")
                return
        except (EOFError, KeyboardInterrupt):
            print("\nSesja przerwana.")
            return
        print()

    print("üöÄ Gotowy do rozmowy! Napisz co≈õ...")
    print("‚îÄ" * 53)

    system_prompt = ("You are Bielik, a helpful Polish AI assistant. "
                     "Respond in Polish unless asked otherwise.")
    messages = [{"role": "system", "content": system_prompt}]

    try:
        while True:
            try:
                user = input("\nüßë Ty: ").strip()
            except EOFError:
                print("\nüëã Do zobaczenia!")
                break

            if not user:
                continue

            # Handle special commands
            if user.startswith(':'):
                if user in [":exit", ":quit", ":q"]:
                    print("üëã Do zobaczenia!")
                    break
                elif user == ":help":
                    show_welcome()
                    continue
                elif user == ":status":
                    status = check_ollama_status()
                    print(f"üîó Status: {status}")
                    continue
                elif user == ":clear":
                    messages = [{"role": "system", "content": system_prompt}]
                    print("üßπ Historia rozmowy wyczyszczona.")
                    continue
                elif user == ":setup":
                    print("üîß Uruchamianie interaktywnej konfiguracji...")
                    interactive_setup()
                    continue
                elif user == ":models":
                    show_hf_models()
                    continue
                elif user.startswith(":download"):
                    parts = user.split(None, 1)
                    if len(parts) > 1:
                        download_hf_model(parts[1])
                    else:
                        print("‚ùì U≈ºyj: :download <nazwa_modelu>")
                    continue
                elif user.startswith(":delete"):
                    parts = user.split(None, 1)
                    if len(parts) > 1:
                        delete_hf_model(parts[1])
                    else:
                        print("‚ùì U≈ºyj: :delete <nazwa_modelu>")
                    continue
                elif user == ":storage":
                    show_storage_stats()
                    continue
                elif user.startswith(":switch"):
                    parts = user.split(None, 1)
                    if len(parts) > 1:
                        switch_model(parts[1])
                    else:
                        print("‚ùì U≈ºyj: :switch <nazwa_modelu>")
                    continue
                else:
                    help_msg = (f"‚ùì Nieznana komenda: {user}. "
                                "Wpisz :help aby zobaczyƒá dostƒôpne komendy.")
                    print(help_msg)
                    continue

            # Process content in user message (URLs, file paths)
            enhanced_user_content = content_processor.process_content_in_text(user)
            
            messages.append({"role": "user", "content": enhanced_user_content})
            print("ü¶Ö Bielik thinking...", end="", flush=True)

            resp = send_chat(messages, model=DEFAULT_MODEL, use_local=use_local_model)
            print("\rü¶Ö Bielik: " + " "*20)  # Clear "thinking" message
            print(f"    {resp}")

            # Only add to history if it's a real response, not an error
            error_prefixes = ["[ERROR]", "[REST ERROR]", "[OLLAMA LIB ERROR]"]
            if not any(resp.startswith(prefix) for prefix in error_prefixes):
                messages.append({"role": "assistant", "content": resp})

    except KeyboardInterrupt:
        print("\n\nüëã Przerwano przez u≈ºytkownika. Do zobaczenia!")
        sys.exit(0)


if __name__ == "__main__":
    main()
