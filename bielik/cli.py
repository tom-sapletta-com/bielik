#!/usr/bin/env python3
"""
bielik CLI - interactive chat shell using Ollama.
Tries REST API first, falls back to `ollama` library if available.
"""

import os
import sys
import requests
import subprocess
import platform
import shutil
import argparse

from .config import get_config, get_logger
from .content_processor import get_content_processor
from .hf_models import get_model_manager, HAS_LLAMA_CPP

# Global configuration and logger
config = get_config()
logger = get_logger(__name__)
content_processor = get_content_processor()
model_manager = get_model_manager()

# try to import official ollama client
try:
    import ollama
    HAVE_OLLAMA = True
except ImportError:
    HAVE_OLLAMA = False


def send_chat(messages, model=None):
    """Send chat messages to Ollama via REST API or ollama lib fallback."""
    if model is None:
        model = config.BIELIK_MODEL
    
    payload = {"model": model, "messages": messages, "stream": False}

    # try REST first
    try:
        resp = requests.post(config.CHAT_ENDPOINT, json=payload, timeout=config.REQUEST_TIMEOUT)
        resp.raise_for_status()
        data = resp.json()
        choice = data.get("choices", [{}])[0]
        msg = choice.get("message", {}).get("content")
        if msg:
            logger.debug(f"REST API successful for model {model}")
            return msg
    except Exception as e:
        logger.warning(f"REST API failed: {e}")

    # fallback: official ollama library
    if HAVE_OLLAMA:
        try:
            response = ollama.chat(model=model, messages=messages)
            logger.debug(f"Ollama library successful for model {model}")
            return response["message"]["content"]
        except Exception as e:
            logger.error(f"Ollama library failed: {e}")
            return f"[OLLAMA LIB ERROR] {e}"

    logger.error("No working Ollama integration available")
    return "[ERROR] No working Ollama integration (REST and lib failed)."


def show_welcome():
    """Display welcome message and help for beginners."""
    print("ğŸ¦… " + "="*50)
    print("   BIELIK - Polski Asystent AI")
    print("   Powered by Ollama + Speakleash")
    print("="*53)
    print()
    print("ğŸ“‹ DostÄ™pne komendy:")
    print("  :help    - pokaÅ¼ tÄ™ pomoc")
    print("  :status  - sprawdÅº poÅ‚Ä…czenie z Ollama")
    print("  :setup   - uruchom interaktywnÄ… konfiguracjÄ™")
    print("  :clear   - wyczyÅ›Ä‡ historiÄ™ rozmowy")
    print("  :exit    - zakoÅ„cz sesjÄ™")
    print("  Ctrl+C   - szybkie wyjÅ›cie")
    print()
    print("ğŸ’¡ WskazÃ³wki:")
    print("  â€¢ Pisz po polsku - Bielik rozumie jÄ™zyk polski!")
    print("  â€¢ Zadawaj pytania, proÅ› o pomoc, rozmawiaj naturalnie")
    print("  â€¢ JeÅ›li Ollama nie dziaÅ‚a, zobaczysz komunikat o bÅ‚Ä™dzie")
    print()


def is_ollama_installed():
    """Check if Ollama binary is installed on the system."""
    return shutil.which("ollama") is not None


def is_ollama_running():
    """Check if Ollama server is running."""
    try:
        resp = requests.get(f"{OLLAMA_HOST.rstrip('/')}/api/tags", timeout=5)
        return resp.status_code == 200
    except Exception:
        return False


def get_installed_models():
    """Get list of installed Ollama models."""
    try:
        resp = requests.get(f"{OLLAMA_HOST.rstrip('/')}/api/tags", timeout=5)
        if resp.status_code == 200:
            data = resp.json()
            return [m['name'] for m in data.get('models', [])]
    except Exception:
        pass
    return []


def is_bielik_model_available():
    """Check if Bielik model is available."""
    models = get_installed_models()
    return any('bielik' in model.lower() for model in models)


def install_ollama():
    """Install Ollama based on the operating system."""
    system = platform.system().lower()
    
    print("ğŸ”§ Instalowanie Ollama...")
    
    try:
        if system == "linux":
            print("ğŸ“¦ Wykryto Linux - uÅ¼ywam curl do pobrania Ollama...")
            cmd = ["curl", "-fsSL", "https://ollama.com/install.sh"]
            result = subprocess.run(cmd, capture_output=True, text=True, timeout=60)
            if result.returncode == 0:
                # Run the install script
                install_cmd = ["sh", "-c", result.stdout]
                subprocess.run(install_cmd, check=True, timeout=300)
                print("âœ… Ollama zainstalowana pomyÅ›lnie!")
                return True
            else:
                print(f"âŒ BÅ‚Ä…d pobierania skryptu instalacji: {result.stderr}")
                return False
                
        elif system == "darwin":  # macOS
            print("ğŸ“¦ Wykryto macOS - sprawdzam Homebrew...")
            if shutil.which("brew"):
                subprocess.run(["brew", "install", "ollama"], check=True, timeout=300)
                print("âœ… Ollama zainstalowana przez Homebrew!")
                return True
            else:
                print("âŒ Homebrew nie znaleziony. Zainstaluj rÄ™cznie z https://ollama.com")
                return False
                
        elif system == "windows":
            print("ğŸ“¦ Wykryto Windows - wymagana instalacja rÄ™czna")
            print("ğŸ“– Pobierz Ollama z: https://ollama.com/download/windows")
            print("   Uruchom pobrany plik .exe i postÄ™puj zgodnie z instrukcjami")
            return False
            
        else:
            print(f"âŒ NieobsÅ‚ugiwany system operacyjny: {system}")
            return False
            
    except subprocess.TimeoutExpired:
        print("âŒ Instalacja przekroczyÅ‚a limit czasu")
        return False
    except subprocess.CalledProcessError as e:
        print(f"âŒ BÅ‚Ä…d podczas instalacji: {e}")
        return False
    except Exception as e:
        print(f"âŒ Nieoczekiwany bÅ‚Ä…d: {e}")
        return False


def start_ollama_server():
    """Start Ollama server in background."""
    try:
        print("ğŸš€ Uruchamianie serwera Ollama...")
        if platform.system().lower() == "windows":
            subprocess.Popen(["ollama", "serve"], creationflags=subprocess.CREATE_NEW_CONSOLE)
        else:
            subprocess.Popen(["ollama", "serve"], stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)
        
        # Wait a bit for server to start
        import time
        time.sleep(3)
        
        if is_ollama_running():
            print("âœ… Serwer Ollama uruchomiony!")
            return True
        else:
            print("âš ï¸ Serwer moÅ¼e potrzebowaÄ‡ wiÄ™cej czasu na uruchomienie...")
            return False
    except Exception as e:
        print(f"âŒ BÅ‚Ä…d uruchamiania serwera: {e}")
        return False


def pull_bielik_model():
    """Download and install Bielik model."""
    try:
        print(f"ğŸ“¥ Pobieranie modelu {DEFAULT_MODEL}...")
        print("â³ To moÅ¼e potrwaÄ‡ kilka minut w zaleÅ¼noÅ›ci od poÅ‚Ä…czenia...")
        
        result = subprocess.run(
            ["ollama", "pull", DEFAULT_MODEL], 
            capture_output=True, 
            text=True, 
            timeout=1800  # 30 minutes timeout
        )
        
        if result.returncode == 0:
            print("âœ… Model Bielik pobrany pomyÅ›lnie!")
            return True
        else:
            print(f"âŒ BÅ‚Ä…d pobierania modelu: {result.stderr}")
            return False
            
    except subprocess.TimeoutExpired:
        print("âŒ Pobieranie modelu przekroczyÅ‚o limit czasu (30 min)")
        return False
    except Exception as e:
        print(f"âŒ BÅ‚Ä…d pobierania modelu: {e}")
        return False


def interactive_setup():
    """Interactive setup process for first-time users."""
    print("ğŸ” Sprawdzanie konfiguracji systemu...")
    print()
    
    setup_needed = False
    
    # Check Ollama installation
    if not is_ollama_installed():
        print("âŒ Ollama nie jest zainstalowana")
        setup_needed = True
        
        try:
            install_choice = input("ğŸ¤” Czy zainstalowaÄ‡ Ollama automatycznie? (t/N): ").strip().lower()
            if install_choice in ['t', 'tak', 'y', 'yes']:
                if install_ollama():
                    print("âœ… Ollama zainstalowana!")
                else:
                    print("âŒ Instalacja nie powiodÅ‚a siÄ™. Zainstaluj rÄ™cznie z https://ollama.com")
                    return False
            else:
                print("ğŸ“– Zainstaluj Ollama rÄ™cznie z: https://ollama.com")
                return False
        except (EOFError, KeyboardInterrupt):
            print("\nâŒ Konfiguracja przerwana przez uÅ¼ytkownika")
            return False
    else:
        print("âœ… Ollama jest zainstalowana")
    
    # Check if Ollama server is running
    if not is_ollama_running():
        print("âŒ Serwer Ollama nie dziaÅ‚a")
        setup_needed = True
        
        try:
            start_choice = input("ğŸ¤” Czy uruchomiÄ‡ serwer Ollama? (t/N): ").strip().lower()
            if start_choice in ['t', 'tak', 'y', 'yes']:
                if not start_ollama_server():
                    print("âŒ Nie udaÅ‚o siÄ™ uruchomiÄ‡ serwera. SprÃ³buj rÄ™cznie: ollama serve")
                    return False
            else:
                print("ğŸ“– Uruchom serwer rÄ™cznie: ollama serve")
                return False
        except (EOFError, KeyboardInterrupt):
            print("\nâŒ Konfiguracja przerwana przez uÅ¼ytkownika")
            return False
    else:
        print("âœ… Serwer Ollama dziaÅ‚a")
    
    # Check if Bielik model is available
    if not is_bielik_model_available():
        print(f"âŒ Model {DEFAULT_MODEL} nie jest dostÄ™pny")
        installed_models = get_installed_models()
        if installed_models:
            print(f"ğŸ“‹ DostÄ™pne modele: {', '.join(installed_models[:5])}")
        setup_needed = True
        
        try:
            model_choice = input(f"ğŸ¤” Czy pobraÄ‡ model {DEFAULT_MODEL}? (t/N): ").strip().lower()
            if model_choice in ['t', 'tak', 'y', 'yes']:
                if not pull_bielik_model():
                    print(f"âŒ Nie udaÅ‚o siÄ™ pobraÄ‡ modelu. SprÃ³buj rÄ™cznie: ollama pull {DEFAULT_MODEL}")
                    return False
            else:
                print(f"ğŸ“– Pobierz model rÄ™cznie: ollama pull {DEFAULT_MODEL}")
                return False
        except (EOFError, KeyboardInterrupt):
            print("\nâŒ Konfiguracja przerwana przez uÅ¼ytkownika")
            return False
    else:
        print(f"âœ… Model {DEFAULT_MODEL} jest dostÄ™pny")
    
    if setup_needed:
        print()
        print("ğŸ‰ Konfiguracja zakoÅ„czona pomyÅ›lnie!")
        print("ğŸš€ Bielik jest gotowy do uÅ¼ycia!")
    else:
        print("âœ… Wszystkie komponenty sÄ… juÅ¼ skonfigurowane!")
    
    return True


def check_ollama_status():
    """Check if Ollama is running and accessible."""
    try:
        # Quick health check
        resp = requests.get(f"{config.OLLAMA_HOST.rstrip('/')}/api/tags", timeout=5)
        if resp.status_code == 200:
            data = resp.json()
            models = [m['name'] for m in data.get('models', [])]
            target_model = config.BIELIK_MODEL
            if any(target_model.lower() in model.lower() for model in models):
                logger.info(f"Ollama status check: Model {target_model} available")
                return f"âœ… Ollama running, model {target_model} available"
            else:
                available = ', '.join(models[:3]) if models else "none"
                logger.warning(f"Ollama running but model {target_model} not found")
                return f"âš ï¸ Ollama running, but model '{target_model}' missing. Available: {available}"
        else:
            logger.error(f"Ollama HTTP error: {resp.status_code}")
            return f"âŒ Ollama responds but HTTP error {resp.status_code}"
    except Exception as e:
        logger.error(f"Ollama connection failed: {e}")
        return f"âŒ Ollama unavailable: {str(e)}"


def parse_args():
    """Parse command line arguments."""
    parser = argparse.ArgumentParser(
        description="Bielik - Polski Asystent AI",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
PrzykÅ‚ady uÅ¼ycia:
  bielik                                    # Standardowa sesja
  bielik --model other-model               # UÅ¼yj innego modelu
  bielik --setup                           # Uruchom interaktywnÄ… konfiguracjÄ™
  bielik --no-setup                        # PomiÅ„ automatycznÄ… konfiguracjÄ™
        """
    )
    
    parser.add_argument(
        "--model", 
        default=DEFAULT_MODEL,
        help=f"Model do uÅ¼ycia (domyÅ›lny: {DEFAULT_MODEL})"
    )
    
    parser.add_argument(
        "--setup", 
        action="store_true",
        help="Uruchom interaktywnÄ… konfiguracjÄ™ systemu"
    )
    
    parser.add_argument(
        "--no-setup", 
        action="store_true",
        help="PomiÅ„ automatycznÄ… konfiguracjÄ™ przy problemach"
    )
    
    parser.add_argument(
        "--host",
        default=OLLAMA_HOST,
        help=f"Adres serwera Ollama (domyÅ›lny: {OLLAMA_HOST})"
    )
    
    return parser.parse_args()


def main():
    args = parse_args()
    
    # Override defaults with command line arguments
    global DEFAULT_MODEL, OLLAMA_HOST, CHAT_ENDPOINT
    DEFAULT_MODEL = args.model
    OLLAMA_HOST = args.host
    CHAT_ENDPOINT = OLLAMA_HOST.rstrip("/") + "/v1/chat/completions"
    
    show_welcome()
    
    # Force setup if requested
    if args.setup:
        print("ğŸ”§ Uruchamianie interaktywnej konfiguracji...")
        print()
        if not interactive_setup():
            print("âŒ Konfiguracja nie powiodÅ‚a siÄ™.")
            return
        print()

    # Check Ollama status at startup
    status = check_ollama_status()
    print(f"ğŸ”— Status: {status}")
    print()

    # Handle problems with automatic setup unless disabled
    if "âŒ" in status and not args.no_setup:
        print("ğŸš¨ UWAGA: Wykryto problemy z konfiguracjÄ…!")
        print()
        
        try:
            setup_choice = input("ğŸ¤” Czy uruchomiÄ‡ automatycznÄ… konfiguracjÄ™? (T/n): ").strip().lower()
            if setup_choice not in ['n', 'nie', 'no']:
                print()
                if interactive_setup():
                    # Recheck status after setup
                    status = check_ollama_status()
                    print(f"ğŸ”— Nowy status: {status}")
                    print()
                else:
                    print("âŒ Automatyczna konfiguracja nie powiodÅ‚a siÄ™.")
        except (EOFError, KeyboardInterrupt):
            print("\nâŒ Konfiguracja przerwana przez uÅ¼ytkownika")
            return

    # If still having issues, offer to continue anyway
    if "âŒ" in status:
        print("ğŸ“– Instrukcje rÄ™cznej naprawy:")
        print("  1. Zainstaluj Ollama: https://ollama.com")
        print("  2. Uruchom: ollama serve")
        print(f"  3. Zainstaluj model: ollama pull {DEFAULT_MODEL}")
        print("  4. SprawdÅº: ollama list")
        print()

        try:
            continue_anyway = input("Czy kontynuowaÄ‡ mimo problemÃ³w? (t/N): ")
            if continue_anyway.lower() not in ['t', 'tak', 'y', 'yes']:
                print("Sesja przerwana. Napraw konfiguracjÄ™ i sprÃ³buj ponownie.")
                return
        except (EOFError, KeyboardInterrupt):
            print("\nSesja przerwana.")
            return
        print()

    print("ğŸš€ Gotowy do rozmowy! Napisz coÅ›...")
    print("â”€" * 53)

    system_prompt = ("You are Bielik, a helpful Polish AI assistant. "
                     "Respond in Polish unless asked otherwise.")
    messages = [{"role": "system", "content": system_prompt}]

    try:
        while True:
            try:
                user = input("\nğŸ§‘ Ty: ").strip()
            except EOFError:
                print("\nğŸ‘‹ Do zobaczenia!")
                break

            if not user:
                continue

            # Handle special commands
            if user.startswith(':'):
                if user in [":exit", ":quit", ":q"]:
                    print("ğŸ‘‹ Do zobaczenia!")
                    break
                elif user == ":help":
                    show_welcome()
                    continue
                elif user == ":status":
                    status = check_ollama_status()
                    print(f"ğŸ”— Status: {status}")
                    continue
                elif user == ":clear":
                    messages = [{"role": "system", "content": system_prompt}]
                    print("ğŸ§¹ Historia rozmowy wyczyszczona.")
                    continue
                elif user == ":setup":
                    print("ğŸ”§ Uruchamianie interaktywnej konfiguracji...")
                    interactive_setup()
                    continue
                else:
                    help_msg = (f"â“ Nieznana komenda: {user}. "
                                "Wpisz :help aby zobaczyÄ‡ dostÄ™pne komendy.")
                    print(help_msg)
                    continue

            # Process content in user message (URLs, file paths)
            enhanced_user_content = content_processor.process_content_in_text(user)
            
            messages.append({"role": "user", "content": enhanced_user_content})
            print("ğŸ¦… Bielik thinking...", end="", flush=True)

            resp = send_chat(messages, model=config.BIELIK_MODEL)
            print("\rğŸ¦… Bielik: " + " "*20)  # Clear "thinking" message
            print(f"    {resp}")

            # Only add to history if it's a real response, not an error
            error_prefixes = ["[ERROR]", "[REST ERROR]", "[OLLAMA LIB ERROR]"]
            if not any(resp.startswith(prefix) for prefix in error_prefixes):
                messages.append({"role": "assistant", "content": resp})

    except KeyboardInterrupt:
        print("\n\nğŸ‘‹ Przerwano przez uÅ¼ytkownika. Do zobaczenia!")
        sys.exit(0)


if __name__ == "__main__":
    main()
