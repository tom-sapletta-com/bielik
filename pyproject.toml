[build-system]
requires = ["setuptools>=61.0", "wheel", "twine", "build"]
build-backend = "setuptools.build_meta"

[project]
name = "bielik"
version = "0.1.7"
description = "Bielik â€” local chat client (CLI + web) with HuggingFace Integration"
authors = [{name = "Tom Sapletta"}]
license = "Apache-2.0"
readme = "README.md"
requires-python = ">=3.8"
keywords = ["ollama", "chat", "llm", "bielik", "polish", "ai", "assistant"]
classifiers = [
    "Development Status :: 4 - Beta",
    "Intended Audience :: Developers",
    "Intended Audience :: End Users/Desktop",
    "Programming Language :: Python :: 3",
    "Programming Language :: Python :: 3.8",
    "Programming Language :: Python :: 3.9",
    "Programming Language :: Python :: 3.10",
    "Programming Language :: Python :: 3.11",
    "Programming Language :: Python :: 3.12",
    "Topic :: Software Development :: Libraries",
    "Topic :: Communications :: Chat",
    "Topic :: Scientific/Engineering :: Artificial Intelligence",
    "Operating System :: OS Independent",
    "Natural Language :: Polish",
    "Natural Language :: English"
]

# MINIMAL TEXT-ONLY DEPENDENCIES (~50MB total)
dependencies = [
  # Core CLI & API functionality
  "fastapi>=0.88.0,<1.0.0",
  "uvicorn[standard]>=0.20.0,<1.0.0", 
  "requests>=2.25.0,<3.0.0",
  "python-dotenv>=0.19.0,<2.0.0",
  
  # Text processing & document analysis  
  "beautifulsoup4>=4.9.0,<5.0.0",
  "html2text>=2020.1.16,<2025.0.0",
  "python-magic>=0.4.24,<1.0.0",
  "pypdf>=3.0.0,<4.0.0",
  "python-docx>=0.8.11,<1.0.0",
  
  # HuggingFace model management (essential for downloading models)
  "huggingface_hub>=0.16.0,<1.0.0"
  
  # HEAVY PACKAGES MOVED TO OPTIONAL DEPENDENCIES:
  # Pillow, transformers, torch (~2GB+) - install with: pip install bielik[vision]
]

[project.optional-dependencies]
# Basic Ollama integration (lightweight)
ollama = ["ollama>=0.1.0,<1.0.0"]

# Local HF model support (CPU-only, medium weight ~100MB)
local = [
  "llama-cpp-python>=0.2.0,<1.0.0",
  "textract>=1.6.3,<1.6.5; sys_platform != 'win32'"
]

# HEAVY VISION & AI PACKAGES (~2GB+) - Install on demand
vision = [
  "Pillow>=9.0.0,<11.0.0",           # Image processing (~5MB)
  "transformers>=4.25.0,<5.0.0",     # HuggingFace models (~100MB)  
  "torch>=1.12.0,<3.0.0"            # PyTorch + CUDA (~2GB!)
]

# GPU acceleration (includes vision + CUDA)
gpu = [
  "bielik[vision]",                   # Include vision packages
  "llama-cpp-python[cuda]>=0.2.0,<1.0.0"
]

# Development tools
dev = [
  "pytest>=6.0.0,<8.0.0",
  "pytest-cov>=2.10.0,<5.0.0", 
  "flake8>=4.0.0,<7.0.0",
  "black>=22.0.0,<24.0.0",
  "isort>=5.10.0,<6.0.0",
  "build>=0.8.0,<1.0.0",
  "twine>=4.0.0,<5.0.0"
]

# Docker support
docker = [
  "docker>=6.0.0,<7.0.0"
]

# Everything (for advanced users)
all = [
  "bielik[ollama,local,vision,dev,docker]"
]

[project.scripts]
bielik = "bielik.cli:main"

[tool.setuptools]
package-dir = {"" = "."}
packages = ["bielik"]
include-package-data = true
