![terminal](terminal.png)
# ğŸ¦… bielik

[![PyPI](https://img.shields.io/pypi/v/bielik.svg)](https://pypi.org/project/bielik/)
[![Python](https://img.shields.io/pypi/pyversions/bielik.svg)](https://www.python.org/)
[![License](https://img.shields.io/badge/license-Apache%202.0-blue.svg)](LICENSE)
[![Build](https://img.shields.io/github/actions/workflow/status/tomsapletta/bielik/python-app.yml?branch=main)](https://github.com/tomsapletta/bielik/actions)
[![Downloads](https://img.shields.io/pypi/dm/bielik.svg)](https://pypi.org/project/bielik/)
[![Code style: flake8](https://img.shields.io/badge/code%20style-flake8-black.svg)](https://flake8.pycqa.org/)
[![Issues](https://img.shields.io/github/issues/tomsapletta/bielik.svg)](https://github.com/tomsapletta/bielik/issues)
[![Stars](https://img.shields.io/github/stars/tomsapletta/bielik.svg)](https://github.com/tomsapletta/bielik/stargazers)
[![Forks](https://img.shields.io/github/forks/tomsapletta/bielik.svg)](https://github.com/tomsapletta/bielik/network)

**Author:** Tom Sapletta  
**License:** Apache-2.0

> ğŸ‡µğŸ‡± **Bielik** is a local chat client for **[Ollama](https://ollama.com)** with CLI and web interfaces, created specifically for the Polish language model **[Bielik](https://huggingface.co/speakleash)** from **[Speakleash](https://speakleash.org/)**.

---

## ğŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                          ğŸ¦… BIELIK                              â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚   ğŸ–¥ï¸  CLI Shell     â”‚  ğŸŒ FastAPI Server   â”‚  ğŸ§ª Test Suite     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚ â€¢ Interactive   â”‚â”‚ â”‚ â€¢ REST /chat      â”‚ â”‚ â”‚ â€¢ Unit tests  â”‚ â”‚
â”‚  â”‚ â€¢ Help system   â”‚â”‚ â”‚ â€¢ WebSocket /ws   â”‚ â”‚ â”‚ â€¢ Mock API    â”‚ â”‚
â”‚  â”‚ â€¢ Cross-platformâ”‚â”‚ â”‚ â€¢ Port 8888       â”‚ â”‚ â”‚ â€¢ CI/CD       â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
            â”‚                       â”‚                       â”‚
            â–¼                       â–¼                       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    ğŸ”„ CONNECTION LAYER                        â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚    REST API (main)   â”‚â—„â”€â”€â–ºâ”‚   Ollama Library (fallback) â”‚  â”‚
â”‚  â”‚ â”Œâ”€ HTTP requests     â”‚    â”‚ â”Œâ”€ ollama.chat()            â”‚  â”‚
â”‚  â”‚ â””â”€ /v1/chat/...      â”‚    â”‚ â””â”€ Direct integration       â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                      â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   ğŸ¦™ OLLAMA SERVER                            â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚ ğŸ“ localhost:11434 (default)                            â”‚  â”‚
â”‚  â”‚ ğŸ¤– Model: bielik (Polish LLM)                           â”‚  â”‚
â”‚  â”‚ ğŸ”— Links: Speakleash â†’ HuggingFace â†’ Ollama             â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ¤– About Bielik Model

**Bielik** is a groundbreaking Polish language model created by **[Speakleash](https://speakleash.org/)** - a foundation dedicated to the development of Polish artificial intelligence.

### ğŸ”— External Dependencies & Links:
- **[Ollama](https://ollama.com)** - Local LLM runtime that hosts the Bielik model
- **[Bielik Model on HuggingFace](https://huggingface.co/speakleash)** - Official model repository
- **[Speakleash Foundation](https://speakleash.org/)** - Creators of the Bielik model
- **[Polish AI Initiative](https://www.gov.pl/web/ai)** - Government support for Polish AI

### ğŸš€ How it Works:
1. **Bielik package** connects to your local **Ollama server**
2. **Ollama** runs the **Bielik model** (downloaded from HuggingFace via Speakleash)
3. **Chat interface** (CLI/Web) sends queries â†’ **Ollama API** â†’ **Bielik model** â†’ responses
4. **Fallback system** ensures connectivity (REST API â†’ ollama library)

---

## ğŸ“Œ Features

- **ğŸ¯ Auto-Setup System** â€” Intelligent first-time setup for beginners
  - **ğŸ” Detection** â€” Automatically detects missing components
  - **ğŸ“¦ Installation** â€” Cross-platform Ollama installation (Linux/macOS)
  - **ğŸš€ Configuration** â€” Downloads and configures Bielik model
  - **ğŸ› ï¸ Interactive** â€” User-friendly prompts and error handling
- **ğŸ–¥ï¸ Enhanced CLI** `bielik` â€” Interactive chat shell with modular architecture
  - **ğŸ“‹ Commands** â€” `:help`, `:status`, `:setup`, `:clear`, `:exit`, `:models`, `:download`, `:delete`, `:switch`
  - **âš™ï¸ Arguments** â€” `--setup`, `--no-setup`, `--model`, `--host`, `--use-local`, `--local-model`
  - **ğŸ¤— HF Integration** â€” Direct Hugging Face model management and local execution
  - **ğŸ”„ Fallback** â€” REST API primary, ollama lib secondary, local models tertiary
  - **ğŸŒ Cross-platform** â€” Windows, macOS, Linux support
- **ğŸ Python API** â€” Programmatic access via `BielikClient` class
  - **ğŸ’¬ Chat methods** â€” `chat()`, `query()`, conversation management
  - **ğŸ”§ System control** â€” Status checking, auto-setup, model management
  - **ğŸ¤— HF Models** â€” Download, manage, and run SpeakLeash models locally
  - **ğŸ“¤ Export** â€” Conversation history in JSON, text, markdown formats
- **ğŸŒ Web Server** (FastAPI on port 8888):  
  - **ğŸ“¡ REST** â€” `POST /chat` endpoint for JSON communication
  - **âš¡ WebSocket** â€” `WS /ws` for real-time chat
  - **ğŸ”„ Fallback** â€” Same dual connectivity as CLI
- **ğŸ§ª Quality Assurance** â€” Comprehensive testing and development tools
  - **âœ… Unit tests** â€” Full coverage with mocked APIs
  - **ğŸ”§ CI/CD** â€” GitHub Actions automation
  - **ğŸ“Š Code quality** â€” Flake8 linting, automated builds  

---

## âš™ï¸ Installation

```bash
pip install bielik
```

Optional dependency (official Ollama lib):

```bash
pip install "bielik[ollama]"
```

---

## ğŸš€ Quick Start Guide

### ğŸ¯ **NEW: Automatic Setup (Recommended)**

The easiest way to get started is with the new automatic setup system:

```bash
# Install Bielik package
pip install bielik

# Start with automatic setup - it will handle everything!
bielik

# Or force setup mode:
bielik --setup
```

The auto-setup system will:
- âœ… **Detect** if Ollama is installed
- âœ… **Install Ollama** automatically (Linux/macOS) 
- âœ… **Start Ollama server** if not running
- âœ… **Download Bielik model** (`SpeakLeash/bielik-7b-instruct-v0.1-gguf`)
- âœ… **Configure** everything for optimal performance

### ğŸ“‹ Manual Setup (Advanced Users)

If you prefer manual control:

#### 1ï¸âƒ£ Install Ollama (Cross-platform)

**Windows:**
```powershell
# Download installer from ollama.com or use winget:
winget install Ollama.Ollama
```

**macOS:**
```bash
# Download .dmg from ollama.com or use Homebrew:
brew install ollama
```

**Linux:**
```bash
# Ubuntu/Debian:
curl -fsSL https://ollama.com/install.sh | sh

# Arch Linux:
sudo pacman -S ollama
```

#### 2ï¸âƒ£ Setup Bielik Model

```bash
# Start Ollama service (Linux/macOS):
ollama serve

# Windows: Ollama starts automatically

# Install Bielik model (new full model name):
ollama pull SpeakLeash/bielik-7b-instruct-v0.1-gguf

# Verify installation:
ollama list
```

#### 3ï¸âƒ£ Install & Use Bielik Package

```bash
# Install from PyPI:
pip install bielik

# Start CLI chat:
bielik

# Skip auto-setup if you prefer manual control:
bielik --no-setup

# Or start web server:
uvicorn bielik.server:app --port 8888
```

---

## ğŸ’» Usage

### ğŸ–¥ï¸ CLI Features & Options

#### Command-Line Arguments (when starting bielik)

```bash
# Basic usage
bielik                                    # Start interactive chat with auto-setup

# Advanced options
bielik --setup                           # Force setup mode
bielik --no-setup                        # Skip automatic setup
bielik --model other-model               # Use different model
bielik --host http://other-host:11434    # Use different Ollama server
bielik --use-local                       # Use local HuggingFace models (bypass Ollama)
bielik --local-model model-name          # Specify local model to use
bielik --help                            # Show all options
```

#### Interactive Commands (inside bielik chat session)

**âš ï¸ Important:** These commands only work **inside** the interactive chat session, not as command-line arguments.

```bash
# Start interactive session first
$ bielik

# Then use these commands inside the chat:
ğŸ§‘ You: :help             # Show help and commands
ğŸ§‘ You: :status           # Check Ollama connection and model availability
ğŸ§‘ You: :setup            # Run interactive setup system
ğŸ§‘ You: :models           # List available and downloaded HuggingFace models
ğŸ§‘ You: :download <model> # Download a SpeakLeash model from Hugging Face
ğŸ§‘ You: :delete <model>   # Delete a downloaded model
ğŸ§‘ You: :switch <model>   # Switch to a different model for execution
ğŸ§‘ You: :storage          # Show model storage statistics
ğŸ§‘ You: :clear            # Clear conversation history
ğŸ§‘ You: :exit             # Quit (or Ctrl+C)
```

#### Usage Examples

```bash
# âœ… Correct - Start with specific model
$ bielik --model SpeakLeash/Bielik-4.5B-v3.0-Instruct-GGUF

# âœ… Correct - Interactive commands inside session
$ bielik
ğŸ§‘ You: :status
ğŸ§‘ You: :switch SpeakLeash/Bielik-4.5B-v3.0-Instruct-GGUF

# âŒ Incorrect - Interactive commands as CLI arguments
$ bielik :status          # This won't work!
$ bielik :switch model    # This won't work!
```

### ğŸ Python API

**NEW:** Use Bielik programmatically in your Python applications:

```python
from bielik.client import BielikClient

# Create client with auto-setup
client = BielikClient()

# Send a message
response = client.chat("How are you?")
print(response)

# Get system status
status = client.get_status()
print(f"Ollama running: {status['ollama_running']}")
print(f"Model available: {status['model_available']}")

# Export conversation
history = client.export_conversation(format="markdown")
```

**Quick functions:**
```python
from bielik.client import quick_chat, get_system_status

# One-off query
response = quick_chat("What is artificial intelligence?")

# Check system without setup
status = get_system_status()
```

**BielikClient Options:**
- `model`: Model name to use
- `host`: Ollama server URL  
- `auto_setup`: Enable/disable automatic setup (default: True)

### Web API

```bash
uvicorn bielik.server:app --port 8888
```

**Endpoints:**
- `POST /chat` - JSON chat endpoint
- `WS /ws` - WebSocket real-time chat

**Example request:**
```json
{"messages": [{"role":"user","content":"Hello!"}]}
```

---

## ğŸ”§ Environment Variables

* `OLLAMA_HOST` â€” default: `http://localhost:11434`
* `BIELIK_MODEL` â€” default: `SpeakLeash/bielik-7b-instruct-v0.1-gguf`

---

## ğŸ› ï¸ Troubleshooting

### Auto-Setup Issues

**Problem:** Auto-setup fails to install Ollama
```bash
# Manual installation required for Windows
# Download from: https://ollama.com/download/windows

# Linux/macOS alternatives:
curl -fsSL https://ollama.com/install.sh | sh  # Linux
brew install ollama                             # macOS
```

**Problem:** Model download fails or times out
```bash
# Try manual download with longer timeout
ollama pull SpeakLeash/bielik-7b-instruct-v0.1-gguf

# Check available disk space (model is ~4GB)
df -h

# Check network connection
curl -I https://ollama.com
```

**Problem:** Ollama server won't start
```bash
# Check if port 11434 is in use
lsof -i :11434  # Linux/macOS
netstat -an | findstr 11434  # Windows

# Try different port
OLLAMA_HOST=http://localhost:11435 ollama serve --port 11435
```

### Runtime Issues

**Problem:** "Connection refused" errors
```bash
# Check Ollama status
bielik :status

bielik :switch SpeakLeash/Bielik-4.5B-v3.0-Instruct-GGUF

# Restart Ollama service
pkill ollama && ollama serve  # Linux/macOS

# Manual server start
ollama serve
```

**Problem:** Model responses are slow
```bash
# Check system resources
htop      # Linux/macOS
taskmgr   # Windows

# Use smaller model if needed
bielik --model llama2  # If available
```

**Problem:** Python API import errors
```bash
# Reinstall with dependencies
pip uninstall bielik
pip install bielik[ollama]

# Check Python path
python -c "import bielik.client; print('OK')"
```

### Getting Help

- **GitHub Issues:** [Report bugs and feature requests](https://github.com/tomsapletta/bielik/issues)
- **Command Help:** `bielik --help` or `:help` in CLI
- **System Status:** Use `:status` command or `get_system_status()` function

---

## ğŸ“ Development

```bash
git clone https://github.com/tomsapletta/bielik.git
cd bielik
python -m venv .venv
source .venv/bin/activate
pip install -e .[ollama]
```


# ğŸ“‚ Package Structure

```
bielik/
â”œâ”€â”€ bielik/
â”‚   â”œâ”€â”€ __init__.py          # Package initialization
â”‚   â”œâ”€â”€ cli.py               # CLI entry point (wrapper)
â”‚   â”œâ”€â”€ client.py            # Client entry point (wrapper)
â”‚   â”œâ”€â”€ server.py            # FastAPI web server
â”‚   â”œâ”€â”€ config.py            # Configuration management
â”‚   â”œâ”€â”€ hf_models.py         # Hugging Face model management
â”‚   â”œâ”€â”€ content_processor.py # Content processing utilities
â”‚   â”œâ”€â”€ cli/                 # Modular CLI components
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ main.py          # Main CLI entry and argument parsing
â”‚   â”‚   â”œâ”€â”€ commands.py      # Command processing and execution
â”‚   â”‚   â”œâ”€â”€ models.py        # HF model management CLI
â”‚   â”‚   â”œâ”€â”€ setup.py         # Interactive setup manager
â”‚   â”‚   â””â”€â”€ send_chat.py     # Chat communication handling
â”‚   â””â”€â”€ client/              # Modular client components
â”‚       â”œâ”€â”€ __init__.py      # Client package exports
â”‚       â”œâ”€â”€ core.py          # Core BielikClient class
â”‚       â”œâ”€â”€ model_manager.py # HF model operations for client
â”‚       â””â”€â”€ utils.py         # Client utility functions
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ test_cli.py          # CLI unit tests
â”‚   â””â”€â”€ test_server.py       # Server unit tests
â”œâ”€â”€ pyproject.toml           # Modern Python packaging
â”œâ”€â”€ setup.cfg                # Package configuration
â”œâ”€â”€ MANIFEST.in              # Package manifest
â”œâ”€â”€ LICENSE                  # Apache 2.0 license
â”œâ”€â”€ README.md                # This documentation
â”œâ”€â”€ Makefile                 # Development automation
â”œâ”€â”€ todo.md                  # Project specifications
â””â”€â”€ .github/workflows/       # CI/CD automation
    â””â”€â”€ python-publish.yml
```



## ğŸ“œ License

[Apache License 2.0](LICENSE)


